{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import glob\n",
    "import joblib\n",
    "# import os\n",
    "\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "CALC_PATHS = '/home/nfs/jludwiczak/af2_cc/af2_multimer/calc'\n",
    "\n",
    "\n",
    "def get_x(id_: int, rank: int, model: str = \"af2\", \n",
    "          use_pairwise: bool = True):\n",
    "\n",
    "    single_repr_fns = sorted(glob.glob(f\"{CALC_PATHS}/{id_}/*_single_repr_rank_00*\"))\n",
    "    pair_repr_fns = sorted(glob.glob(f\"{CALC_PATHS}{id_}/*_pair_repr_rank_00*\"))\n",
    "\n",
    "    mat = np.load(single_repr_fns[rank]).mean(axis=0)\n",
    "    if use_pairwise:\n",
    "        mat = np.hstack((mat, np.load(pair_repr_fns[rank]).mean(axis=0).mean(axis=0)))\n",
    "    return mat\n",
    "\n",
    "def get_af2_emb(id_: int, model_id: int, use_pairwise: bool):\n",
    "\n",
    "    single_repr_fns = sorted(glob.glob(f\"{CALC_PATHS}/{id_}/*_single_repr_rank_*_model_{model_id+1}_*\"))\n",
    "    pair_repr_fns = sorted(glob.glob(f\"{CALC_PATHS}/{id_}/*_pair_repr_rank_*_model_{model_id+1}_*\"))\n",
    "\n",
    "\n",
    "    mat = np.load(single_repr_fns[0]).mean(axis=0)\n",
    "\n",
    "    if use_pairwise:\n",
    "        mat = np.hstack((mat, np.load(pair_repr_fns[0]).mean(axis=0).mean(axis=0)))\n",
    "\n",
    "    return mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"tests/set4_homooligomers.csv\", sep=\"\\t\")\n",
    "df = df.drop_duplicates(subset=\"full_sequence\", keep=\"first\")\n",
    "df.parallel.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "def train(c=10, balanced=0, dual=1, ensemble_size=1, use_pairwise=True, use_scaler=True):\n",
    "\n",
    "    df = pd.read_csv(\"tests/set4_homooligomers.csv\", sep=\"\\t\")\n",
    "    df = df.drop_duplicates(subset=\"full_sequence\", keep=\"first\")\n",
    "    results = np.zeros((ensemble_size, 5, len(df), 2))\n",
    "    model = {}\n",
    "    probabilities = []\n",
    "    for j in range(0, ensemble_size):\n",
    "        for i in range(0, 5): # 5 since we have 5 AF2 models\n",
    "\n",
    "            X = np.asarray([get_af2_emb(id_, model_id=i, use_pairwise=use_pairwise) for id_ in df.index])\n",
    "            y = df['parallel'].values\n",
    "            cv = KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "            for k, (tr_idx, te_idx) in enumerate(cv.split(X, y)):\n",
    "\n",
    "                X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "                y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "                if use_scaler == 1:\n",
    "                    sc = StandardScaler()\n",
    "                    X_tr = sc.fit_transform(X_tr)\n",
    "                    X_te = sc.transform(X_te)\n",
    "                    model[f\"scaler_{j}_{i}_{k}\"] = sc\n",
    "                clf = LogisticRegression(C=c, max_iter=1000, solver='liblinear',\n",
    "                                         dual = False if dual == 0 else True, \n",
    "                                         class_weight = 'balanced' if balanced == 1 else None) \n",
    "                clf.fit(X_tr, y_tr)\n",
    "                results[j, i, te_idx] = clf.predict_proba(X_te)\n",
    "                model[f\"clf_{j}_{i}_{k}\"] = clf\n",
    "\n",
    "    y_pred_bin = results.mean(axis=0).mean(axis=0).argmax(axis=1)\n",
    "    results_ = {}\n",
    "    results_[\"accuracy\"] = accuracy_score(y, y_pred_bin)\n",
    "    results_[\"f1\"] = f1_score(y, y_pred_bin, average='macro')\n",
    "\n",
    "    df[\"y_pred\"] = y_pred_bin\n",
    "    # df[\"prob_dimer\"] = results.mean(axis=0).mean(axis=0)[:, 0]\n",
    "    # df[\"prob_trimer\"] = results.mean(axis=0).mean(axis=0)[:, 1]\n",
    "    # df[\"prob_tetramer\"] = results.mean(axis=0).mean(axis=0)[:, 2]\n",
    "\n",
    "    return results_, model, df\n",
    "\n",
    "# c = [1,5,10,15,20]\n",
    "# balanced = [0,1]\n",
    "# dual = [0,1]\n",
    "# ensemble_size = [1,2,3,4,5]\n",
    "# use_pairwise = [True, False]\n",
    "\n",
    "# results = []\n",
    "# for c_, balanced_, dual_, ensemble_size_, use_pairwise_ in product(c, balanced, dual, ensemble_size, use_pairwise):\n",
    "#     results_, model, df = train(c=c_, balanced=balanced_, dual=dual_, ensemble_size=ensemble_size_, use_pairwise=use_pairwise_)\n",
    "#     print(results_[\"accuracy\"], results_[\"f1\"])\n",
    "#     results.append((c_, balanced_, dual_, ensemble_size_, use_pairwise_, results_[\"accuracy\"], results_[\"f1\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(c=10, balanced=0, dual=1, ensemble_size=1, use_pairwise=True, use_scaler=True):\n",
    "    df = pd.read_csv(\"tests/set4_homooligomers.csv\", sep=\"\\t\")\n",
    "    df = df.drop_duplicates(subset=\"full_sequence\", keep=\"first\")\n",
    "    \n",
    "    # Initialize results arrays for both target variables\n",
    "    results_parallel = np.zeros((ensemble_size, 5, len(df), 2))\n",
    "    results_oligo = np.zeros((ensemble_size, 5, len(df), 3))\n",
    "    \n",
    "    model = {}\n",
    "    for j in range(0, ensemble_size):\n",
    "        for i in range(0, 5): # 5 since we have 5 AF2 models\n",
    "            X = np.asarray([get_af2_emb(id_, model_id=i, use_pairwise=use_pairwise) for id_ in df.index])\n",
    "            \n",
    "            # Combine target variables into a single array\n",
    "            y_parallel = df['parallel'].values\n",
    "            le = LabelEncoder()\n",
    "            df['oligo_state'] = le.fit_transform(df['chains'])\n",
    "            y_state = df['oligo_state'].values\n",
    "            y = np.column_stack((y_parallel, y_state))\n",
    "            \n",
    "            cv = KFold(n_splits=5, shuffle=True)\n",
    "            for k, (tr_idx, te_idx) in enumerate(cv.split(X, y)):\n",
    "                X_tr, X_te = X[tr_idx], X[te_idx]\n",
    "                y_tr, y_te = y[tr_idx], y[te_idx]\n",
    "\n",
    "                if use_scaler == 1:\n",
    "                    sc = StandardScaler()\n",
    "                    X_tr = sc.fit_transform(X_tr)\n",
    "                    X_te = sc.transform(X_te)\n",
    "                    model[f\"scaler_{j}_{i}_{k}\"] = sc\n",
    "                \n",
    "                # Train and evaluate multi-output model\n",
    "                clf = MultiOutputClassifier(LogisticRegression(C=c, max_iter=2000, solver='liblinear',\n",
    "                                         dual=False if dual == 0 else True,\n",
    "                                         class_weight='balanced' if balanced == 1 else None))\n",
    "                clf.fit(X_tr, y_tr)\n",
    "                proba_parallel, proba_chains = clf.predict_proba(X_te)\n",
    "                results_parallel[j, i, te_idx] = proba_parallel\n",
    "                results_oligo[j, i, te_idx] = proba_chains\n",
    "                model[f\"clf_{j}_{i}_{k}\"] = clf\n",
    "\n",
    "    # Calculate average probabilities and predicted classes for both target variables\n",
    "    avg_proba_parallel = results_parallel.mean(axis=0).mean(axis=0)\n",
    "    avg_proba_state = results_oligo.mean(axis=0).mean(axis=0)\n",
    "    \n",
    "    y_pred_bin_parallel = avg_proba_parallel.argmax(axis=1)\n",
    "    y_pred_bin_state = avg_proba_state.argmax(axis=1)\n",
    "    \n",
    "    results_ = {}\n",
    "    \n",
    "    # Calculate accuracy and F1 score for parallel target variable\n",
    "    results_[\"accuracy_parallel\"] = accuracy_score(y_parallel, y_pred_bin_parallel)\n",
    "    results_[\"f1_parallel\"] = f1_score(y_parallel, y_pred_bin_parallel, average='macro')\n",
    "    \n",
    "    # Calculate accuracy and F1 score for chains target variable\n",
    "    results_[\"accuracy_oligo_state\"] = accuracy_score(y_state, y_pred_bin_state)\n",
    "    results_[\"f1_oligo_state\"] = f1_score(y_state, y_pred_bin_state, average='macro')\n",
    "\n",
    "    df[\"y_pred_parallel\"] = y_pred_bin_parallel\n",
    "    df[\"y_pred_chains\"] = y_pred_bin_state\n",
    "    df[\"prob_dimer\"] = avg_proba_state[:,0]\n",
    "    df[\"prob_trimer\"] = avg_proba_state[:,1]\n",
    "    df[\"prob_tetramer\"] = avg_proba_state[:, 2]\n",
    "    df[\"prob_parallel\"] = avg_proba_parallel[:, 1]\n",
    "    df[\"prob_antiparallel\"] = avg_proba_parallel[:, 0]\n",
    "    df.to_csv('model_results.csv')\n",
    "\n",
    "    return results_, model, df  \n",
    "\n",
    "\n",
    "train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"model/results.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import  seaborn as sns\n",
    "confusion_matrix(df.parallel, df.y_pred_parallel)\n",
    "# sns.heatmap(confusion_matrix(df.parallel, df.y_pred_parallel), annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "confusion_matrix(df.oligo_state, df.y_pred_chains)\n",
    "sns.heatmap(confusion_matrix(df.oligo_state, df.y_pred_chains), annot=True, cmap=\"Blues\", fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"new_model_results.csv\")\n",
    "le = LabelEncoder()\n",
    "df['parallel_code'] = le.fit_transform(df['parallel'])\n",
    "df[['pdb','parallel_code','parallel']].loc[df.pdb == '4dzo'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from src.predictor import predict_oligo_state_and_topology\n",
    "import pandas as pd\n",
    "\n",
    "# path = glob.glob('tests/data/*/')\n",
    "# df = pd.DataFrame()\n",
    "# for f in path:\n",
    "#     df = pd.concat([df, predict_oligo_state_and_topology(f, use_pairwise=True)], axis=0)\n",
    "# df.to_csv('tests/test_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf  = pd.DataFrame()\n",
    "for f in path:\n",
    "    tdf = pd.concat([tdf, predict_oligo_state_and_topology(f, use_pairwise=True)], axis=0)\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('tests/test_df.csv').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.predictor import predict_oligo_state_and_topology\n",
    "df = pd.DataFrame()\n",
    "\n",
    "test_cases = [x for x in glob.glob('/home/nfs/rmadaj/IDUB/dc2_oligo/tmp/*/') if 'env' not in x]\n",
    "pdb_annot = [x.split('/')[-2].split('_')[0] for x in glob.glob('tmp/*/*_env/')]\n",
    "test_cases\n",
    "for pdb, test_case in zip(pdb_annot,test_cases):\n",
    "    try:\n",
    "        tdf = predict_oligo_state_and_topology(test_case, use_pairwise=True)\n",
    "        tdf['pdb'] = pdb\n",
    "        df = pd.concat([df, tdf], axis=0)\n",
    "    except:\n",
    "        print(test_case, pdb)\n",
    "        pass\n",
    "df  = df.sort_values(by=['pdb'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from biopandas.pdb import PandasPdb\n",
    "import sys\n",
    "sys.path.append('/home/nfs/sdunin/scr/localpdb/')\n",
    "from localpdb import PDB\n",
    "from Bio.Data.IUPACData import protein_letters_3to1\n",
    "import gzip\n",
    "\n",
    "topdf = pd.read_pickle('/home/nfs/rmadaj/pdb_scan_nr_topology_20200717.p')\n",
    "tdf = topdf.drop_duplicates(subset=['pdb']).reset_index(drop=True)\n",
    "pdbs = tdf.head(2000)[['pdb']].values.flatten()\n",
    "\n",
    "lpdb = PDB('/home/db/localpdb/', version=20210716)\n",
    "input_dict = {}\n",
    "\n",
    "for pdb in pdbs:\n",
    "    entry = lpdb.entries.loc[lpdb.entries.index==pdb].pdb_fn.values[0]\n",
    "    seq = ''.join(PandasPdb().read_pdb(entry).amino3to1()['residue_name'])\n",
    "    if len(seq) > 300:\n",
    "        continue\n",
    "    if len(seq) < 30:\n",
    "        continue\n",
    "    if '?' in seq:\n",
    "        continue\n",
    "    input_dict[pdb] = seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# i = 0\n",
    "# for key, value in input_dict.items():\n",
    "#     print(key, value)\n",
    "#     os.makedirs(f'tmp/{i}', exist_ok=True)\n",
    "#     seq = value\n",
    "#     pdb = key\n",
    "    \n",
    "#     with open(f'tmp/{i}/{key}_monomer.csv', 'w') as f:\n",
    "#         f.write('id,sequence\\n')\n",
    "#         f.write(f'{pdb},{seq}\\n')\n",
    "        \n",
    "#     with open(f'tmp/{i}.sh', 'w') as f:\n",
    "#         f.write('#!/bin/bash\\n')\n",
    "#         f.write(f'cd ./{i}\\n')\n",
    "#         f.write('source /opt/miniconda3/bin/activate cf_1.5\\n')\n",
    "#         f.write(f'colabfold_batch {key}_monomer.csv . --num-models 5 --model-type alphafold2_multimer_v3 --num-recycle 5 --save-single-representations --save-pair-representations\\n')\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(f'tmp/batch.sh', 'w') as f:\n",
    "#     f.write(\"\"\"#!/bin/bash\n",
    "# #SBATCH -p gpu\n",
    "# #SBATCH -n 4\n",
    "# #SBATCH --exclude=edi0[6-8]\n",
    "# #SBATCH --gres=gpu:1\n",
    "# #SBATCH --mem=16GB\n",
    "# #SBATCH -J dc2_bench\n",
    "# #SBATCH --array 0-417\n",
    "\n",
    "# bash $SLURM_ARRAY_TASK_ID.sh\n",
    "\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = tdf.loc[tdf.pdb.isin(input_dict.keys())].reset_index(drop=True)\n",
    "tdf['oligo_state'] = tdf.toplogy_len.apply(lambda x: 0 if x == 2 else 1 if x == 3 else 2)\n",
    "tdf['parallel'] = tdf.topology_class.apply(lambda x: 0 if '_0' in  x else 1)\n",
    "tdf = tdf.sort_values(by=['pdb'])\n",
    "tdf\n",
    "tempdf =  df.loc[df.pdb.isin(tdf.pdb)].sort_values(by=['pdb'])\n",
    "tdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tempdf.loc[tempdf.pdb.isin(tdf.pdb.values)]\n",
    "df_compare = pd.read_pickle('/home/nfs/rmadaj/pdb_scan_nr_topology_20200717.p')\n",
    "df_compare = df_compare.loc[df_compare.pdb.isin(df.pdb.values)].reset_index(drop=True).drop_duplicates(subset=['bundle_id']).reset_index(drop=True)\n",
    "df_compare['oligo_state'] = df_compare.toplogy_len.apply(lambda x: 0 if x == 2 else 1 if x == 3 else 2)\n",
    "df_compare['parallel'] = df_compare.topology_class.apply(lambda x: 0 if '_0' in  x else 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = df_compare.sort_values(by=['pdb'])\n",
    "\n",
    "# for df_compare check what values of 'oligo state' per pdb match with df\n",
    "for i, row in df_compare.iterrows():\n",
    "    if row.pdb in df.pdb.values:\n",
    "        if row.oligo_state == df.loc[df.pdb == row.pdb].y_pred_oligo.values[0]:\n",
    "            print(row.pdb, row.oligo_state, df.loc[df.pdb == row.pdb].y_pred_oligo.values[0])\n",
    "        else:\n",
    "            print(row.pdb, row.oligo_state, df.loc[df.pdb == row.pdb].y_pred_oligo.values[0], 'WRONG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare.to_csv('df_compare.csv', index=False)\n",
    "df.to_csv('df_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "cm1 = confusion_matrix(df.y_pred_oligo, df_compare.oligo_state)\n",
    "sns.heatmap(cm1, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
    "#accuracy\n",
    "cm1.diagonal().sum()/cm1.sum()\n",
    "\n",
    "# cm2 = confusion_matrix(df.y_pred_parallel, tdf.parallel)\n",
    "# sns.heatmap(cm2, annot=True, cmap=\"Blues\", fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = pd.read_csv('tests/set4_homooligomers.csv', sep = \"\\t\")\n",
    "# calculate average length of full sequence\n",
    "ddf['full_sequence_len'] = ddf.full_sequence.apply(lambda x: len(x))\n",
    "ddf.full_sequence_len.mean()\n",
    "ddf.full_sequence_len.median()\n",
    "ddf.full_sequence_len.max()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "util",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
